{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import QuantileRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.c_[np.ones(100), np.random.normal(0, 5, size=(100, 5))]\n",
    "beta = np.array([10, 1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = X @ beta + np.random.normal(0, 1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantileRegression:\n",
    "    def __init__(self, theta, verbose=False):\n",
    "        self.theta = theta\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def fit(self, X, y, learning_rate=0.001, num_iter=1000):\n",
    "        self.dim = X.shape[1]\n",
    "        self.num_iter = num_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.params = np.random.uniform(0, 1, self.dim)\n",
    "        self.gradient_descent(X, y)\n",
    "    \n",
    "    def sum_abs_loss(self, y, y_hat):\n",
    "        dev = y - y_hat\n",
    "        return np.sum(np.maximum(self.theta * dev, (self.theta - 1) * dev))\n",
    "    \n",
    "    def gradient_descent(self, X, y):\n",
    "        self.count = 0\n",
    "        y_hat = X @ self.params\n",
    "        best_loss = self.sum_abs_loss(y, y_hat)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f'{self.count}/{self.num_iter}:', best_loss)\n",
    "        \n",
    "        self.best_params = np.copy(self.params)\n",
    "\n",
    "        for i in range(self.num_iter):\n",
    "            # update params\n",
    "            grads = self.gradient(X, y, self.params)\n",
    "            self.params -= grads * self.learning_rate\n",
    "            \n",
    "            # print loss\n",
    "            y_hat = X @ self.params\n",
    "            self.count += 1\n",
    "            loss = self.sum_abs_loss(y, y_hat)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f'{self.count}/{self.num_iter}:', loss)\n",
    "            \n",
    "            if loss < best_loss:\n",
    "                self.best_params = np.copy(self.params)\n",
    "                best_loss = loss\n",
    "\n",
    "    def gradient(self, X, y, params):\n",
    "        y_hat = X @ self.params\n",
    "        dev = y - y_hat\n",
    "        grads = np.zeros(self.dim)\n",
    "        \n",
    "        # this gradient for sum(|y - xb|) only\n",
    "        for i in range(len(grads)):\n",
    "            grads[i] = np.sum(np.where(dev > 0,\n",
    "                                       - self.theta * np.sign(X[:, i] * self.params[i]),\n",
    "                                       (1 - self.theta) * np.sign(X[:, i] * self.params[i])))\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.64708373  1.00918346  1.71656298  3.08060967  3.48084359  4.32914151]\n",
      "[7.84778997 0.95973058 2.06227284 2.96423484 4.02349655 4.94364497]\n",
      "[3.35677069 1.00386861 2.05566202 3.11606055 3.84708736 4.41111839]\n",
      "[8.55834054 0.99959593 1.99106566 2.97300592 4.03515452 5.0014718 ]\n",
      "[-16.49892749   1.08413108   2.15388939   2.76508997   4.07227252\n",
      "   3.76337485]\n",
      "[9.41706562 1.01233848 2.01723498 3.000608   3.96325221 5.04944315]\n",
      "[10.15348854  1.00506953  1.97174569  3.02026303  3.99561627  5.07963791]\n",
      "[10.12990076  0.99071535  1.96491419  3.02241105  3.99358358  5.06107899]\n",
      "[10.67124141  1.02889153  2.03328617  3.03575993  3.9795603   5.09724803]\n",
      "[10.5818026   1.00718279  1.99175597  3.03488802  3.99034049  5.0387914 ]\n",
      "[11.91995448  1.02421135  2.11585989  3.00662923  3.95939441  5.11728873]\n",
      "[11.66465655  1.06820416  2.02541937  3.05573165  4.00386565  5.0026522 ]\n",
      "[12.24137519  1.06615256  2.11507001  2.99171828  3.97967561  5.11776363]\n",
      "[12.11976851  0.98919757  1.97207873  3.06047571  3.98733738  5.13912004]\n"
     ]
    }
   ],
   "source": [
    "for q in [0.01, 0.05, 0.2, 0.5, 0.7, 0.95, 0.99]:\n",
    "    qr = QuantileRegression(q)\n",
    "    qr.fit(X, y)\n",
    "    print(qr.params)\n",
    "    \n",
    "    qr_sklearn = QuantileRegressor(quantile=q, alpha=0, fit_intercept=False)\n",
    "    qr_sklearn.fit(X,y)\n",
    "    print(qr_sklearn.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
