{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import linprog\n",
    "from sklearn.linear_model import QuantileRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.c_[np.ones(100), np.random.normal(0, 5, size=(100, 5))]\n",
    "beta = np.array([10, 1, 2, 3, 4, 5])\n",
    "y = X @ beta + np.random.normal(0, 1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantileRegression:\n",
    "    def __init__(self, theta, verbose=False):\n",
    "        self.theta = theta # theta-quantile\n",
    "        self.verbose = verbose # to print the loss\n",
    "        \n",
    "    def fit(self, X, y, learning_rate=0.001, tol=1e-10):\n",
    "        self.n, self.p = X.shape\n",
    "        self.params = np.random.uniform(0, 1, self.p)\n",
    "        \n",
    "        # initialize the params for gradient descent\n",
    "        self.tol = tol\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.gradient_descent(X, y)\n",
    "    \n",
    "    def loss(self, y, y_hat):\n",
    "        # sum of absolute error\n",
    "        dev = y - y_hat\n",
    "        return np.sum(np.maximum(self.theta * dev, (self.theta - 1) * dev))\n",
    "    \n",
    "    def gradient_descent(self, X, y):\n",
    "        y_hat = X @ self.params\n",
    "        loss = self.loss(y, y_hat)\n",
    "\n",
    "        while True:\n",
    "            # update params\n",
    "            grads = self.gradient(X, y, self.params)\n",
    "            self.params -= grads * self.learning_rate\n",
    "            \n",
    "            # print loss\n",
    "            y_hat = X @ self.params\n",
    "            current_loss = self.loss(y, y_hat)\n",
    "            if loss - current_loss < self.tol:\n",
    "                break\n",
    "            else:\n",
    "                loss = current_loss\n",
    "                \n",
    "    def gradient(self, X, y, params):\n",
    "        y_hat = X @ self.params\n",
    "        dev = y - y_hat\n",
    "        grads = np.zeros(self.p)\n",
    "        \n",
    "        # this gradient for sum(|y - xb|) only\n",
    "        for i in range(len(grads)):\n",
    "            xbeta = X[:, i] * self.params[i]\n",
    "            xbeta = np.where(abs(xbeta) < 1, xbeta, np.sign(xbeta))\n",
    "            grads[i] = np.sum(np.where(dev > 0,\n",
    "                                       self.theta * -xbeta, \n",
    "                                       (self.theta-1) * -xbeta))\n",
    "        return grads\n",
    "    \n",
    "    def recursively_fit(self, returns, learning_rate=0.001, tol=1e-10):\n",
    "        # initialize the params for gradient descent\n",
    "        self.tol = tol\n",
    "        self.learning_rate = learning_rate\n",
    "        self.p = 4\n",
    "        self.params = np.random.uniform(0, 1, 4)\n",
    "        loss = np.inf\n",
    "        while True:\n",
    "            sigmas = self.asymmetric_slope(returns, self.params)\n",
    "            constant_term = np.ones_like(returns)\n",
    "            X = np.c_[constant_term, sigmas[:-1], np.maximum(returns, 0), np.minimum(returns, 0)]\n",
    "            y = sigmas[1:]\n",
    "            \n",
    "            # update params\n",
    "            grads = self.gradient(X, y, self.params)\n",
    "            self.params -= grads * self.learning_rate\n",
    "            \n",
    "            # print loss\n",
    "            y_hat = X @ self.params\n",
    "            current_loss = self.loss(y, y_hat)\n",
    "            if loss - current_loss < self.tol:\n",
    "                break\n",
    "            else:\n",
    "                loss = current_loss\n",
    "        \n",
    "\n",
    "    def asymmetric_slope(self, returns, params):\n",
    "        b1, b2, b3, b4 = params\n",
    "        sigmas = np.zeros(returns.shape[0]+1)\n",
    "        sigmas[0] = np.std(returns) # im not sure\n",
    "        for t in range(1, len(sigmas)):\n",
    "            sigmas[t] = b1 + b2 * sigmas[t-1] + max(b3 * returns[t-1], 0) + b4 * min(b3 * returns[t-1], 0)\n",
    "        return sigmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "qr = QuantileRegression(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "qr.recursively_fit(np.array(range(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.56951569, 0.60021941, 0.36394334, 0.51556232])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qr.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
